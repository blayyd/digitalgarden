---
{"dg-publish":true,"permalink":"/linear-algebra/04-vector-arithmetic/4-5-algebraic-properties-of-vector-transposes/","tags":["Type/Theorem","Topic/Linear_Algebra"]}
---

Types: *N/A*
Examples: *N/A*
Constructions: *N/A*
Generalizations: [[Linear Algebra/04 Vector Arithmetic/4.4 Transpose of a Vector\|4.4 Transpose of a Vector]]

Properties: *N/A*
Sufficiencies: *N/A*
Questions: *N/A*

> [!theorem|*] Algebraic Properties of Vector Transposes
> 
> Let $\vec{x},\vec{y} \in \mathbb{R}^{n}$ and $a,b \in \mathbb{R}$. Then, all of the following are properties of [[Linear Algebra/04 Vector Arithmetic/4.2 Column Vector Addition\|4.2 Column Vector Addition]].
> 1. $(\vec{x}^{T})^{T}= \vec{x}$
> 2. $(\vec{x}+\vec{y})^{T} = \vec{x}^{T}+\vec{y}^{T}$
> 3. $(a \cdot \vec{x})^{T} = a \cdot [\vec{x}^{T}]$

`\begin{proof}`
Let $\vec{x}, \vec{y} \in \mathbb{R}^{n}$ and $a \in \mathbb{R}$. We start off by proving our first property. Consider:
$$
(\vec{x}^{T})^{T} = \left(\begin{bmatrix}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{bmatrix}^{T}\right)^{T} = \begin{bmatrix}
x_{1} & x_{2} & \dots & x_{n}
\end{bmatrix}^{T} = \begin{bmatrix}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{bmatrix} = \vec{x}
$$
This shows that $(\vec{x}^{T})^{T} = \vec{x}$. Next, let's prove that the transpose of a sum of vectors is equal to the sum of their transposes. Consider:
$$
\begin{align*}
(\vec{x}+\vec{y})^{T} &= \left(\begin{bmatrix}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{bmatrix} + \begin{bmatrix}
y_{1} \\
y_{2} \\
\vdots \\
y_{n}
\end{bmatrix}\right)^{T}\\
&= \begin{bmatrix}
x_{1}+y_{1} \\
x_{2}+y_{2} \\
\vdots \\
x_{n}+y_{n}
\end{bmatrix}^{T}\\
&= 
\begin{bmatrix}
x_{1}+y_{1} & x_{2}+y_{2} & \dots & x_{n}+y_{n}
\end{bmatrix}\\
&= \begin{bmatrix}
x_{1}&x_{2}&\dots&x_{n}
\end{bmatrix} + \begin{bmatrix}
y_{1}&y_{2}&\dots&y_{n}
\end{bmatrix}\\
&= \vec{x}^{T} +\vec{y}^{T}
\end{align*}
$$
This shows  $(\vec{x}+\vec{y})^{T} = \vec{x}^{T}+\vec{y}^{T}$. Finally, let's prove scalar multiplication goes through the transpose operation. Consider:
$$
\begin{align*}
(a \cdot \vec{x})^{T} &= \left(a\cdot \begin{bmatrix}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{bmatrix}\right)^{T}\\
&= \begin{bmatrix}
a\cdot x_{1} \\
a\cdot x_{2} \\
\vdots \\
a\cdot x_{n}
\end{bmatrix}^{T}\\
&= \begin{bmatrix}
a\cdot x_{1} & a\cdot x_{2} & \dots & a\cdot x_{n}
\end{bmatrix}\\
&= a \cdot \begin{bmatrix}
x_{1} & x_{2} & \dots & x_{n}
\end{bmatrix}\\
&= a \cdot \vec{x}^{T}
\end{align*}
$$
This completes our proof.
`\end{proof}`
**Remark.** Although the properties are shown with column vectors, the same holds true if $\vec{x},\vec{y}$ were row vectors. As we will see later, the transpose operation is extremely helpful in re-interpreting dot products between vectors as matrix-matrix multiplication. Transposes are also helpful in formulating outer-products between vectors.
 <span style='float:right;'>$\blacklozenge$</span>