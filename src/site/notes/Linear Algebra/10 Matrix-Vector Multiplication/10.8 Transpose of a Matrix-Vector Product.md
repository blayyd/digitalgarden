---
{"date created":"Wednesday, May 15th 2024, 2:40:03 pm","date modified":"Wednesday, May 15th 2024, 4:41:06 pm","time spent":null,"tags":["Type/Theorem","Topic/Linear_Algebra"],"links":"[[10 Matrix-Vector Multiplication]]","dg-publish":true,"permalink":"/linear-algebra/10-matrix-vector-multiplication/10-8-transpose-of-a-matrix-vector-product/","dgPassFrontmatter":true}
---

Types: *N/A*
Examples: *N/A*
Constructions: *N/A*
Generalizations: *N/A*

Properties: *N/A*
Sufficiencies: *N/A*
Questions: *N/A*

> [!theorem|*] The Transpose of a Matrix-Vector Product
> 
> Let matrix $A \in \mathbb{R}^{m \times n}$, let [[Linear Algebra/03 Vector Modeling/3.1 Column Vector\|3.1 Column Vector]] $\vec{x} \in \mathbb{R}^{n \times 1}$, and $\vec{y} \in \mathbb{R}^{m \times 1}$. Then, each of the following statements holds true:
> 1. $(A \vec{x})^{T} = \vec{x}^{T} A^{T}$
> 2. $(\vec{y}^{T}A)^{T}=A^{T}\vec{y}$

To prove this theorem, we will start with a proposition and develop proofs for that proposition. Recall that math statements are in the form $P \implies Q$.
> [!proposition] 
> $$
> \underbrace{ \text{If } A \in \mathbb{R}^{m \times n} \text{ and } \vec{x} \in \mathbb{R}^{n} }_{ P }, \text{ then } \underbrace{ (A \vec{x})^{T} = \vec{x}^{T} A^{T} }_{ Q }
> $$

For our first proposition, we will develop two different proofs. 
1. First proof will rely on [[Linear Algebra/10 Matrix-Vector Multiplication/10.2 Matrix-Column-Vector Multiplication via Dot Products\|10.2 Matrix-Column-Vector Multiplication via Dot Products]]
2. Second proof will rely on [[Linear Algebra/10 Matrix-Vector Multiplication/10.1 Matrix-Column-Vector Multiplication via Linear Combinations\|10.1 Matrix-Column-Vector Multiplication via Linear Combinations]]

`\begin{proof}`
Let $A \in \mathbb{R}^{m \times n}$ and $\vec{x} \in \mathbb{R}^{n \times 1}$. We define two different vectors
$$
\begin{align}
 & \vec{b}=A\vec{x} & \text{ and } &  & \vec{r}=\vec{x}^{T}A^{T}.
\end{align}
$$
Our goal is to prove $\vec{b}^{T} = \vec{r}$. We want to prove this via an entry-by-entry approach, which is why we are the dot product definition of matrix-column-vector multiplication. Recall that
$$
\begin{align*}
b_{k}=\text{entry}_{(1,k)}(\vec{b}^{T}) &= \text{entry}_{(k,1)}(\vec{b})\\
&= \text{entry}(_{k,1}) (A \vec{x})\\
&= (\text{row}_{k}(A))^{T} \cdot \vec{x}\\
&= (A(k,:))^{T} \cdot \vec{x}\\
&= \sum_{j=1}^{n} a_{kj}x_{j}
\end{align*}
$$
Each term of the sum is the product between two scalars. By our algebraic property of commutativity, we can rearrange the order in which we execute multiplication. We can rewrite our summation as
$$
\begin{align*}
b_{k} &= \sum_{j=1}^{n} a_{kj}x_{j}\\
&= \vec{x} \cdot (A(k,:))^{T}\\
&= \vec{x} \cdot (A^{T}(:,k))\\
&= \text{entry}_{(1,k)} (\vec{x}^{T} A^{T})=r_{k}
\end{align*}
$$
This shows what we wanted. We can conclude $(A\vec{x})^{T} = \vec{x}^{T} A^{T}$.
`\end{proof}`

Our first proof is based on the entry-by-entry definition of matrix-column-vector multiplication. Another way we can prove our proposition is through linear combinations.
`\begin{proof}`
Let $A \in \mathbb{R}^{m \times n}$ and $\vec{x} \in \mathbb{R}^{n \times 1}$. We define two different vectors
$$
\begin{align}
 & \vec{b}=A\vec{x} & \text{ and } &  & \vec{r}=\vec{x}^{T}A^{T}.
\end{align}
$$
Our goal is to prove that $\vec{b}^{T}=\vec{r}$. We want to prove this via vector operations i.e. linear combinations. Recall that
$$
\begin{align*}
\vec{b}^{T}=(A\vec{x})^{T} &= \left( \sum_{k=1}^{n}x_{k} A(:,k) \right)^{T}\\
&= \sum_{k=1}^{n} (x_{k}A(:,k))^{T}\\
&= \sum_{k=1}^{n} x_{k} (A(:,k))^{T}\\
&= \sum_{k=1}^{n} x_{k}A^{T}(k,:)\\
&= \vec{x}^{T}A^{T}
\end{align*}
$$
This is what we wanted to show. We can conclude that $(A\vec{x})^{T} = \vec{x}^{T}A^{T}$.
`\end{proof}`

---

> [!proposition] 
> $$
> \underbrace{ \text{If } A \in \mathbb{R}^{m \times n} \text{ and } \vec{y} \in \mathbb{R}^{m} }_{ P }, \text{ then } \underbrace{ (\vec{y}^{T}A)^{T}=A^{T}\vec{y} }_{ Q }
> $$

TODO:
- [ ] Add proof in the future