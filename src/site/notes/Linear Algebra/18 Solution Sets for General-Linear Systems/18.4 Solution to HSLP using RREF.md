---
{"dg-publish":true,"permalink":"/linear-algebra/18-solution-sets-for-general-linear-systems/18-4-solution-to-hslp-using-rref/","tags":["Type/Theorem","Topic/Linear_Algebra"]}
---

Types: *N/A*
Examples: *N/A*
Constructions: *N/A*
Generalizations: [[Linear Algebra/18 Solution Sets for General-Linear Systems/18.3 Homogenous Linear System\|18.3 Homogenous Linear System]]

Properties: *N/A*
Sufficiencies: *N/A*
Questions: *N/A*

> [!theorem|*] Solution to $A \cdot \textcolor{red}{\vec{x}}= \vec{0}$ using $\text{RREF}$
> 
> Let $A \in \mathbb{R}^{m \times n}$ be a "given" matrix and suppose $U = \text{RREF}(A)$. Then, for an $\textcolor{red}{\vec{x}}\in \mathbb{R}^{n}$ we have
> $$
> A \cdot \textcolor{red}{\vec{x}} = \vec{0} \iff U \cdot \textcolor{red}{\vec{x}} = \vec{0}.
> $$

`\begin{proof}`
Let $A \in \mathbb{R}^{m \times n}$. Suppose $U = \text{RREF}(A)$. To prove this biconditional theorem, we need to show:
1. If $A \cdot \textcolor{red}{\vec{x}} = \vec{0}$, then $U \cdot \textcolor{red}{\vec{x}}=\vec{0}$.
2. If $U \cdot \textcolor{red}{\vec{x}}=\vec{0}$, then $A \cdot \textcolor{red}{\vec{x}} = \vec{0}$.

To start, recall our algorithm for creating $\text{RREF}(A)$. By construction of [[Linear Algebra/17 General Linear-Systems/17.3 Reduced Row Echelon Form\|17.3 Reduced Row Echelon Form]] matrix $U$, there exists a sequence of elementary matrices $E_{1}, E_{2}, \dots, E_{t} \in \mathbb{R}^{m \times m}$ such that
$$
E_{t} \cdot E_{t-1} \cdots E_{2} \cdot E_{1} \cdot A  =U
$$
for some $t \in \mathbb{N}$. Moreover, each elementary matrix $E_{j}$ can either be written as a [[Linear Algebra/09 Matrix Arithmetic/9.8 Shear Matrix\|9.8 Shear Matrix]], [[Linear Algebra/09 Matrix Arithmetic/9.9 Dilation Matrix\|9.9 Dilation Matrix]], or [[Linear Algebra/09 Matrix Arithmetic/9.10 Transposition Matrix\|9.10 Transposition Matrix]] (AKA permutation matrix), for $1 \leq j \leq t$. Because each of these matrices is nonsingular, then the $m \times m$ matrix
$$
E = E_{t} \cdot E_{t-1} \cdots  E_{2} \cdot E_{1}
$$
is nonsingular. This follows from the fact that any product of nonsingular matrices is also nonsingular.

Let's prove condition 1 of this proof, the forward direction.
$$
\begin{align}
A \cdot \textcolor{red}{\vec{x}} = \vec{0}  &  & \implies  &  & E \cdot A \cdot \textcolor{red}{\vec{x}}  & = E \cdot \vec{0} \\
 &  & \implies  &  & U \cdot \textcolor{red}{\vec{x}}  & = \vec{0}
\end{align}
$$
The final line follows since for $E \cdot  \vec{0}= \vec{0}$ for any $E \in \mathbb{R}^{m \times m}$. Now, let's prove condition 2, the backwards direction.
$$
\begin{align}
U \cdot \textcolor{red}{\vec{x}} = \vec{0}  &  & \implies (E \cdot A) \cdot \textcolor{red}{\vec{x}}  & = E \cdot  \vec{0} \\
 &  & \implies E \cdot (A \cdot \textcolor{red}{\vec{x}})  & = \vec{0} \\
 &   &  \implies  A \cdot \textcolor{red}{\vec{x}}  & = \vec{0}
\end{align}
$$
The final line results from the fact that if $E \in \mathbb{R}^{m \times m}$ is nonsingular, then we know $E \cdot \vec{y} = \vec{0}$ iff $\vec{y}= \vec{0}$. In this case, we set $\vec{y}=A \cdot \textcolor{red}{\vec{x}}$. With this, we have finished our proof in both directions, both of which hold true.

`\end{proof}`