---
{"dg-publish":true,"permalink":"/linear-algebra/16-determinants/16-4-deriving-the-determinant-function/","tags":["Topic/Linear_Algebra","Type/Exercise"]}
---

Types: *N/A*
Examples: *N/A*
Constructions: *N/A*
Generalizations: *N/A*

Properties: *N/A*
Sufficiencies: *N/A*
Questions: *N/A*

> [!exercise|*] Deriving the Determinant Function
> 
> Can we make some guesses about how the determinant function would look like?


We want the determinant function to give us some information on whether the input matrix is nonsingular or not.
$$
\det(I_{n}) = 1 \neq 0
$$
1 would be a nice output and not 0 because for nonsingularity, we hope that the matrix has [[Linear Algebra/06 Span and Linear Independence/6.7 Linearly Independent Vectors\|linearly independent columns]]. We also need to figure out the "cost" of multiplying by elementary matrices.
$$
\begin{align*}
\det (S_{ik}(c) \cdot I_{n}) &= \\
\det (D_{j}(c) \cdot I_{n} ) &= \\
\det (P_{ik} \cdot I_{n}) &= \\
\implies \det(E \cdot A) &= 
\end{align*}
$$
Let's look at a singular $2 \times 2$ matrix.
$$
\begin{bmatrix}
1 & 2 \\
0 & 0
\end{bmatrix}
$$
This matrix is singular because
$$
\begin{align*}
\text{column}_{2}(A) &= 2 \cdot \text{column}_{1}(A)\\
\implies \begin{bmatrix}
2 \\
0
\end{bmatrix} &= 2 \begin{bmatrix}
1 \\
0
\end{bmatrix}\\
\implies A(:,2) &= 2 \cdot A(:,1)\\
\implies \begin{bmatrix}
0 \\
0
\end{bmatrix} &= 2 \cdot A(:,1) + -1 \cdot A(:,2)\\
\implies A \begin{bmatrix}
2 \\
-1
\end{bmatrix} &= \vec{0}
\end{align*}
$$
However, we are working in vector space. How can we determine if the matrix is invertible through scalars? 
- Recall [[Calculus 3/04 Cross Products in 3D/4.3 Area of Parallelogram\|4.3 Area of Parallelogram]] from Math 1C
- & If the two vectors are parallel $\parallel$, the area of the parallelogram is 0. If they are nonparallel, the area is nonzero.
- ? How do we measure that area?
	- $x_{1}y_{2}-x_{2}y_{1}$ 

For area of the parallelogram, we see
$$
\det (\begin{bmatrix}
x_{1} & y_{1} \\
x_{2} & y_{2}
\end{bmatrix}) = x_{1} y_{2} - x_{2}y_{1}
$$
Therefore, 
$$
\implies \det \left( \begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix} \right) = a_{11}a_{22} - a_{21}a_{12}
$$
Let's try this with $2 \times 2$ elementary matrices.
$$
\begin{align*}
\det(S_{21}(c)) &= \det \left( \begin{bmatrix}
1 & 0 \\
c & 1
\end{bmatrix} \right) =1\\
\det \left( D_{1}(c) \right)  &= \det \left( \begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix} \right) =c\\
\det(P_{12})  &= \det \left( \begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix} \right) =-1
\end{align*}
$$
We can generalize this into a conjecture.

> [!conjecture] 
> $$
> \begin{align*}
> \det(S_{ik}(c) \cdot A) &=  \det(A)\\
> \det(D_{j}(c) \cdot A) &= c \cdot \det (A)\\
> \det (P_{ik}A) &= -\det (A)
> \end{align*}
> $$

This gives us insight into the cost of multiplying by each elementary matrix.