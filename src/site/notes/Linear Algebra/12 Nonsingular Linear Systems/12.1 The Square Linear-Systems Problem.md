---
{"dg-publish":true,"permalink":"/linear-algebra/12-nonsingular-linear-systems/12-1-the-square-linear-systems-problem/","tags":["Type/Definition","Topic/Linear_Algebra","calc"]}
---

Types: *N/A*
Examples: *N/A*
Constructions: *N/A*
Generalizations: *N/A*

Properties: [[Linear Algebra/12 Nonsingular Linear Systems/12.8 Solution Set to Square Linear-Systems Problem\|12.8 Solution Set to Square Linear-Systems Problem]]
Sufficiencies: *N/A*
Questions: *N/A*

> [!definition|*] The Square Linear-Systems Problem
> 
> Let $n \in \mathbb{N}$. Let $A \in \mathbb{R}^{n \times n}$ be a "given" square, nonsingular matrix. Let $\vec{b} \in \mathbb{R}^{n}$ be a "given" vector. Then, the square linear-systems problem (AKA the NLSP) is to find an unknown vector $\textcolor{red}{\vec{x}} \in \mathbb{R}^{n }$ such that
> $$
> A \cdot \textcolor{red}{\vec{x}} = \vec{b}
> $$
> - Note that this is a backwards problem.
{ #2987f6}


**Remark.** For now, we can think of a nonsingular matrix as having [[Linear Algebra/06 Span and Linear Independence/6.7 Linearly Independent Vectors\|linearly independent]] columns. In other words, the individual columns of $A$ are unable to be created from [[Linear Algebra/06 Span and Linear Independence/6.1 Linear Combination of Vectors\|linear combinations]] of the other columns.
 <span style='float:right;'>$\blacklozenge$</span>

---

# Relation to Matrix-Vector Multiplication Problem
Let the function $f: \mathbb{R}^{n}\mapsto \mathbb{R}^{N}$ such that $f(\vec{x}) = A \cdot \vec{x}$. 
$$
\begin{align*}
\text{Domain}(f) &= \mathbb{R}^{n}\\
\text{Codomain}(f) &= \mathbb{R}^{n}\\
\text{Rng}(f) &= \{ \vec{b} \in \mathbb{R}^{n}: \vec{b} = A \cdot \vec{x} \text{ for some } \vec{x} \in \mathbb{R}^{n} \}
\end{align*}
$$
Recall that an equivalent for the range of our [[Linear Algebra/10 Matrix-Vector Multiplication/10 Matrix-Vector Multiplication\|10 Matrix-Vector Multiplication]] function $f$ can be also be given by
$$
\text{Rng}(f) = \text{Span} \{ A(:,k) \}_{k=1}^{n}
$$
> [!tldr] 
> We say $\vec{b}$ is in the range of $f$ iff we can write $\vec{b}$ as a linear combination of the columns of matrix $A$.

In contrast, when solving the square linear-systems problem, we need to create our square matrix $A$ and a vector $\vec{b}$. We then need to *work backwards* to calculate all possible vectors $\vec{x}$ such that 
$$
A\vec{x} = \vec{b}
$$
or conclude that no $\vec{x}$ exists. In other words, the linear-systems problem is the *inverse* of the matrix-vector multiplication problem.