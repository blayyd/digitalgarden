---
{"dg-publish":true,"permalink":"/linear-algebra/15-lu-factorization-without-pivoting/15-2-review-of-nonsingular-gravity-model/","tags":["Type/Example","Topic/Linear_Algebra"]}
---

Types: *N/A*
Examples: *N/A*
Constructions: *N/A*
Generalizations: [[Linear Algebra/15 LU Factorization without Pivoting/15.1 LU Factorization without Pivoting\|15.1 LU Factorization without Pivoting]]

Properties: *N/A*
Sufficiencies: *N/A*
Questions: *N/A*

> [!example|*] Review of Nonsingular Gravity Model
> 
> Let's look back at our [[Linear Algebra/12 Nonsingular Linear Systems/12.2.1 Nonsingular Matrix to Model Gravity\|12.2.1 Nonsingular Matrix to Model Gravity]]. 
> $$
> \begin{bmatrix}
> 1 & 3.0 & 9.00 \\
> 1 & 3.3 & 10.89 \\
> 1 & 3.6 & 12.96
> \end{bmatrix}\textcolor{red}{\begin{bmatrix}
> x_{1} \\
> x_{2} \\
> x_{3}
> \end{bmatrix}} = \begin{bmatrix}
> 3.000 \\
> 2.559 \\
> 1.236
> \end{bmatrix}
> $$

Recall that the first thing we did was annihilate our first column step-by step.
$$
\underbrace{ \begin{bmatrix}
1 & 0 & 0 \\
-1 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} }_{ S_{21}(-1) } \begin{bmatrix}
1 & 3.0 & 9.00 \\
1 & 3.3 & 10.89 \\
1 & 3.6 & 12.96
\end{bmatrix} \textcolor{red}{\begin{bmatrix}
x_{1} \\
x_{2} \\
x_{3}
\end{bmatrix}}
$$
After annihilating that value, we moved down and annihilated the value under.
$$
\underbrace{ \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
-1 & 0 & 1
\end{bmatrix} }_{ S_{31}(-1) }
\underbrace{ \begin{bmatrix}
1 & 0 & 0 \\
-1 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} }_{ S_{21}(-1) } \begin{bmatrix}
1 & 3.0 & 9.00 \\
1 & 3.3 & 10.89 \\
1 & 3.6 & 12.96
\end{bmatrix} \textcolor{red}{\begin{bmatrix}
x_{1} \\
x_{2} \\
x_{3}
\end{bmatrix}}
$$
After that, we moved to pivot 2.
$$
\underbrace{ \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & -2 & 1
\end{bmatrix} }_{ S_{32}(-2) } \underbrace{ \begin{bmatrix}
1 & 3.0 & 9.00 \\
0 & 0.3 & 1.89 \\
0 & 0.6 & 3.96
\end{bmatrix} }_{ S_{31}(-1) \cdot S_{21}(-1) \cdot A }
$$
Notice that
$$
\begin{align*}
S_{31}(-1) \cdot S_{21}(-1) &= \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
-1 & 0 & 1
\end{bmatrix} \begin{bmatrix}
1 & 0 & 0 \\
-1 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}\\
&= (I_{3} + -1 \cdot \vec{e}_{3} \vec{e}_{1}^{T}) \cdot (I_{3} + -1 \vec{e}_{2} \cdot \vec{e}_{1}^{T})\\
&= I_{3} + -1 \cdot \vec{e}_{3} \cdot \vec{e}_{1}^{T}+ (I_{3}+-1 \vec{e}_{3} \vec{e}_{1}^{T} ) \cdot - 1 \cdot \vec{e}_{2} \vec{e}_{1}^{T}\\
&= I_{3} + -1 \vec{e}_{3} \vec{e}_{1}^{T} + -1 \vec{e}_{2} \vec{e}_{1}^{T} + -1 \vec{e}_{3} \vec{e}_{1}^{T} \cdot -1 \cdot \vec{e}_{2} \cdot \vec{e}_{1}^{T}\\
&= I_{3} + -1 \cdot \vec{e}_{3} \cdot \vec{e}_{1}^{T} + -1 \cdot \vec{e}_{2} \cdot \vec{e}_{1}^{T} + 1 \vec{e}_{3} (\vec{e}_{1}^{T} \cdot \vec{e}_{2}) \vec{e}_{1}^{T}\\
\implies S_{31}(-1) \cdot S_{21}(-1) &= I_{3} + -1 \cdot \vec{e}_{2} \cdot \vec{e}_{1}^{T}+ -1 \cdot \vec{e}_{2} \cdot \vec{e}_{1}^{T}\\
&= \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} + \begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
-1 & 0 & 0
\end{bmatrix} + \begin{bmatrix}
0 & 0 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}\\
\implies \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
-1 & 0 & 1
\end{bmatrix} \cdot \begin{bmatrix}
1 & 0 & 0 \\
-1 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}&= \begin{bmatrix}
1 & 0 & 0 \\
-1 & 1 & 0 \\
-1 & 0 & 1
\end{bmatrix}
\end{align*}
$$
For such a complex calculation that matrix multiplication is, we seem to get an elegant answer. It looks like the two shear matrices just overlapped on top of each other. Unfortunately, this doesn't always work.
$$
S_{32}(-2) \cdot S_{31}(-1) \cdot S_{21}(-1) \neq \begin{bmatrix}
1 & 0 & 0 \\
-1 & 1 & 0 \\
-1 & -2 & 1
\end{bmatrix}
$$
Let's do the calculation to see why.
$$
\begin{align*}
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & -2 & 1
\end{bmatrix} \begin{bmatrix}
1 & 0 & 0 \\
-1 & 1 & 0 \\
-1 & 0 & 1
\end{bmatrix} &= \begin{bmatrix}
1 & 0 & 0 \\
-1 & 1 & 0 \\
1 & -2 & 1
\end{bmatrix}
\end{align*}
$$
The stuff that happens in pivot 2 stays separate from the stuff that happens in pivot 2. 
$$
\begin{align*}
\underbrace{ \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & -2 & 1
\end{bmatrix} }_{ L_{2} } \underbrace{ \begin{bmatrix}
1 & 0 & 0 \\
-1 & 1 & 0 \\
-1 & 0 & 1
\end{bmatrix} }_{ L_{1} } \underbrace{ \begin{bmatrix}
1 & 3.0 & 9.00 \\
1 & 3.3 & 10.89 \\
1 & 3.6 & 12.96
\end{bmatrix} }_{ A } &= \underbrace{ \begin{bmatrix}
1 & 3.0 & 9.00 \\
0 & 0.3 & 1.89 \\
0 & 0 & 0.18
\end{bmatrix} }_{ U }
\end{align*}
$$
Recall the definition 
<div class="transclusion internal-embed is-loaded"><a class="markdown-embed-link" href="/linear-algebra/09-matrix-arithmetic/9-12-gauss-transform/#013e14" aria-label="Open link"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a><div class="markdown-embed">



> [!definition|*] Gauss Transform
> 
> Let natural numbers $n,k \in \mathbb{N}$ with $k < n$. Let [[Linear Algebra/03 Vector Modeling/3.1 Column Vector\|3.1 Column Vector]]  $\underset{ \text{tau} }{ \vec{\tau} } \in \mathbb{R}^{n}$ whose first $k$ components are zero. Suppose $\vec{\tau}$ is in the form
> $
> \vec{\tau}^{T } = \begin{bmatrix}
> \underbrace{ {0\:\:\:\dots \: \: \:0} }_{ k } & \tau_{k+1} & \dots & \tau_{n}
> \end{bmatrix}
> $
> Then, a **Gauss transformation** is a matrix
> $
> L_{k} = I_{n} - \vec{\tau} \cdot \vec{e}_{k}^{T}
> $
> We call the vector $\vec{\tau}$ a **Gauss vector**.

</div></div>

Let's apply this definition to our example.
$$
\begin{align*}
L_{1} = \begin{bmatrix}
1 & 0 & 0 \\
-1 & 1 & 0 \\
-1 & 0 & 1
\end{bmatrix} &=  \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} + \begin{bmatrix}
0 & 0 & 0 \\
-1 & 0 & 0 \\
-1 & 0 & 0
\end{bmatrix}\\
&= \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}-\begin{bmatrix}
0 & 0 & 0 \\
1 & 0 & 0 \\
1 & 0 & 0
\end{bmatrix}\\
&= \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} - \begin{bmatrix}
0 \\
1 \\
1
\end{bmatrix}\begin{bmatrix}
1 & 0 & 0
\end{bmatrix}\\
\implies L_{1} &= I_{3}-\vec{\tau}_{1} \vec{e}_{1}^{T} \text{ with } \vec{\tau}_{1}=\begin{bmatrix}
0 \\
1 \\
1
\end{bmatrix}=\begin{bmatrix}
0 \\
\frac{a_{21}}{a_{11}} \\
\frac{a_{31}}{a_{11}}
\end{bmatrix}
\end{align*}
$$
> [!question] Why are we even using a Gauss Transform?
> The whole idea is we're trying to solve a nonsingular linear systems problem by turning our modeling matrix into [[Linear Algebra/08 Anatomy of Matrices/8.9 Upper-Triangular Matrix\|8.9 Upper-Triangular Matrix]]. The whole process of turning our matrix into upper-triangular is multiplying $A$ on the left by a sequence of elementary matrices. In this case, our matrix is a [[Linear Algebra/12 Nonsingular Linear Systems/12.3 Regular Matrix\|12.3 Regular Matrix]]. The trend that we notice is that the shear matrices for each pivot is "married" into one. This matrix is known as the [[Linear Algebra/09 Matrix Arithmetic/9.12 Gauss Transform\|9.12 Gauss Transform]]. Instead of using a bunch of [[Linear Algebra/09 Matrix Arithmetic/9.8 Shear Matrix\|9.8 Shear Matrix]], we can put the $c$ values into the same column.

We solve our NLSP from left to right. 
$$
L_{2} \cdot L_{1} \cdot A = U
$$
To get rid of $L_{2}$ first, we can multiply it by its [[Linear Algebra/13 Invertible Matrices/13.1 Inverse of a Square Matrix\|13.1 Inverse of a Square Matrix]].
$$
L_{2}^{-1} \cdot L_{2} \cdot L_{1} \cdot A = L_{2}^{-1} \cdot U
$$
- ! Remember if we multiply on the left by inverse, we need to do the same on the other side!

Once we've gotten rid of $L_{2}$, we want to get rid of $L_{1}$.
$$
\begin{align*}
L_{1}^{-1} \cdot L_{2}^{-1} \cdot L_{2} \cdot L_{1} \cdot A &=  L_{1}^{-1}\cdot L_{2}^{-1}\cdot U\\
\implies L_{1}^{-1} \cdot I_{3} \cdot L_{1} \cdot A &= L_{1}^{-1}\cdot L_{2}^{-1}\cdot U\\
\implies L_{1}^{-1}\cdot L_{1} \cdot A &= L_{1}^{-1}\cdot L_{2}^{-1}\cdot U\\
\implies I_{3} \cdot A &= L_{1}^{-1}\cdot L_{2}^{-1}\cdot U\\
\implies A &= L_{1}^{-1}\cdot L_{2}^{-1}\cdot U
\end{align*}
$$
Note the inverse of our first Gauss transform:
$$
\begin{align*}
L_{1}^{-1} &= (I_{3}-\vec{\tau}_{1}\vec{e}_{1}^{T})^{-1}\\
&= (I_{3}+\vec{\tau}_{1}\vec{e}_{1}^{T})
\end{align*}
$$
We can write a more general case as:
$$
L_{k}^{-1} = (I_{n}-\vec{\tau}_{k}\vec{e}_{k}^{T})^{-1} = (I_{n} + \vec{\tau}_{k}\vec{e}_{k}^{T})
$$

The first Gauss transform zeroes out the entries underneath the first pivot. The second Gauss transform zeroes out the entries underneath the second pivot. We can generalize, saying a Gauss transform zeroes out the entries underneath the $k$th pivot.
$$
\begin{align*}
\implies L_{1}^{-1} \cdot L_{2}^{-1} &= \begin{bmatrix}
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 0 & 1
\end{bmatrix} \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 2 & 1
\end{bmatrix}\\
&= \begin{bmatrix}
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 2 & 1
\end{bmatrix} =L
\end{align*}
$$
- & Because the matrices are in this order, we can simply "marry" the two matrices!

We now have our $L$ matrix for $LU$ factorization.
$$
\begin{align*}
\underbrace{ \begin{bmatrix}
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 2 & 1
\end{bmatrix} }_{ L } \underbrace{ \begin{bmatrix}
1 & 3.0 & 9.00 \\
0 & 0.3 & 1.89 \\
0 & 0 & 0.18
\end{bmatrix} }_{ U }&=  \underbrace{ \begin{bmatrix}
1 & 3.0 & 9.00 \\
1 & 3.3 & 10.89 \\
1 & 3.6 & 12.96
\end{bmatrix} }_{ A }\\
\implies A\vec{x}=\vec{b} &\iff (L \cdot U) \textcolor{red}{\vec{x}} = \vec{b}\\
& \iff L \cdot (U \cdot \textcolor{red}{\vec{x}})=\vec{b}\\
&\iff \underset{ \text{forward substitution} }{ L\textcolor{red}{\vec{y}}=\vec{b} }, \underset{ \text{backwards substitution} }{ U\textcolor{red}{\vec{x}}=\vec{y}B }

\end{align*}
$$
Let's see $LU$ factorization in action by starting with [[Linear Algebra/12 Nonsingular Linear Systems/12.5 Forward Substitution\|12.5 Forward Substitution]].
$$
\begin{align*}
\begin{bmatrix}
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 2 & 1
\end{bmatrix} \textcolor{red}{\begin{bmatrix}
y_{1} \\
y_{2} \\
y_{3}
\end{bmatrix}} &=  \begin{bmatrix}
3.000 \\
2.559 \\
1.236
\end{bmatrix}\\
\implies y_{1} &= 3\\
\implies 1 \cdot y_{1} + 1 \cdot \textcolor{red}{y_{2}} &= 2.559\\
\implies \textcolor{red}{y_{2}} = \frac{1}{1}(2.559-1(3))&= -0.441\\
\implies 1 \cdot y_{1} + 2 \cdot y_{2} + 1 \cdot \textcolor{red}{y_{3}}&= 1.236\\
\implies \textcolor{red}{y_{3}}=\frac{1}{1}(1.236-2(-0.441)-3)&= -0.882
\end{align*}
$$
Therefore,
$$
\begin{align}
&\implies L \textcolor{red}{\vec{y}} = \vec{b} \implies \vec{y} = \begin{bmatrix}
3.000 \\
-0.441 \\
-0.882
\end{bmatrix} \\
&\implies U \textcolor{red}{\vec{x}} = \vec{y} \iff \begin{bmatrix}
1 & 3.0 & 9.00 \\
0 & 0.3 & 1.89 \\
0 & 0 & 0.18
\end{bmatrix} \textcolor{red}{\begin{bmatrix}
x_{1} \\
x_{2} \\
x_{3}
\end{bmatrix}}= \begin{bmatrix}
3.000 \\
-0.441 \\
-0.882
\end{bmatrix}
\end{align}
$$
We can solve this using [[Linear Algebra/12 Nonsingular Linear Systems/12.2 Backwards Substitution\|12.2 Backwards Substitution]].
$$
\begin{align*}
\implies x_{3} \cdot 0.18 &=  -0.882 \\
x_{3} &= -4.9\\
\implies x_{2} \cdot 0.3 + x_{3} \cdot 1.89 &= -0.441\\
x_{2} &= 29.4\\
\implies 1 \cdot x_{1} + 3 \cdot x_{2} + 9 \cdot x_{3} &= 3\\
x_{1} &= -41.1\\
\implies \begin{bmatrix}
x_{1} \\
x_{2} \\
x_{3}
\end{bmatrix} &= \begin{bmatrix}
-41.1 \\
29.4 \\
-4.9
\end{bmatrix}
\end{align*}
$$
This is the same solution we found in [[Linear Algebra/12 Nonsingular Linear Systems/12.2.1 Nonsingular Matrix to Model Gravity\|12.2.1 Nonsingular Matrix to Model Gravity]].