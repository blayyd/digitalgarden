---
{"dg-publish":true,"permalink":"/linear-algebra/10-matrix-vector-multiplication/10-6-relation-between-matrix-vector-multiplication-and-matrix-partitions/","tags":["Type/Proposition","Topic/Linear_Algebra"]}
---

Types: *N/A*
Examples: *N/A*
Constructions: *N/A*
Generalizations: *N/A*

Properties: *N/A*
Sufficiencies: *N/A*
Questions: *N/A*

> [!proposition|*] Relation Between Matrix-Vector Multiplication and Matrix Partitions
> Recall the different methods of doing matrix-vector multiplication. These are related to the different partitions of a matrix.
> - [[Linear Algebra/08 Anatomy of Matrices/8.13 Column Partition of a Matrix\|8.13 Column Partition of a Matrix]]
> 	- [[Linear Algebra/10 Matrix-Vector Multiplication/10.1 Matrix-Column-Vector Multiplication via Linear Combinations\|10.1 Matrix-Column-Vector Multiplication via Linear Combinations]]
> 	- [[Linear Algebra/10 Matrix-Vector Multiplication/10.2 Matrix-Column-Vector Multiplication via Dot Products\|10.2 Matrix-Column-Vector Multiplication via Dot Products]]
> - [[Linear Algebra/08 Anatomy of Matrices/8.14 Row Partition of a Matrix\|8.14 Row Partition of a Matrix]]
> 	- [[Linear Algebra/10 Matrix-Vector Multiplication/10.4 Row-Vector-Matrix Multiplication via Linear Combinations\|10.4 Row-Vector-Matrix Multiplication via Linear Combinations]]
> 	- [[Linear Algebra/10 Matrix-Vector Multiplication/10.5 Row-Vector-Matrix Multiplication via Dot Products\|10.5 Row-Vector-Matrix Multiplication via Dot Products]]

Below is a graphic illustrating the relationships. 

<div class="transclusion internal-embed is-loaded"><div class="markdown-embed">

<div class="markdown-embed-title">

# 500

</div>



==⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠==


# Text Elements
scalarized version

(data broken into
individual entries) 
Matrix-Row-Vector
Multiplication 
vectorized version

(data chunked into
vector-sized pieces) 
Matrix-Column-Vector
Multiplication 
# Embedded files
8a9f80f541b321794f4fbec2aa21051b8b369096: $A \cdot \vec{x} = \vec{b}$
349d20c01f1a6ab00d324e2a24cc4a73821c82e6: $A \cdot \vec{x} = \vec{b}$
3e06897a5bcc06638c3bbd111a63bdb567acd4ce: $\vec{b} = \sum_{i=1}^{m} x_i A(i,:)$
52edc1c69e12a2be439256dd86297f5d7485dad2: $\vec{b} = \sum_{k=1}^{n} x_k A(:,k)$
c5a456f1fdd62c390b2cb8e5d19140e22588a04a: $b_i = [A(i,:)]^{T} \cdot \vec{x}$
83e11f75cea9f244c233ffae4c8d4a19b28238fe: $= \sum_{k=1}^{n} a_{ik} x_k$
a7977a63a2b20179c5351d1c4672ee07a9705369: $b_k =  \vec{x} \cdot A(:,k)$
fa620616aafcbc35868c5b0e2d6b71b6fa51d28f: $= \sum_{i=1}^{m} x_i a_{ik}$
c0958aeb128d030012bb43043ad2ae2160041e06: $\vec{x}^T \cdot A = \vec{b}$
c110adcb1bf0c6e31a02a0fba332ce3d63afdbee: $\vec{x}^T \cdot A = \vec{b}$



</div></div>


Notice:
- The modeling matrix is on the left for matrix-column-vector multiplication but is on the right for matrix-row-vector multiplication.
- The linear combination methods vectorize the data while the dot product methods focus on scalarized data

When solving real world problems that require us to decide which method to use, two useful questions to ask are:
1. Is the modeling matrix the left or right factor?
2. Do we want vectorized or scalarized data?

---
**Remark.** The relation between the methods will prove useful when solving computer science problems.
 <span style='float:right;'>$\blacklozenge$</span>