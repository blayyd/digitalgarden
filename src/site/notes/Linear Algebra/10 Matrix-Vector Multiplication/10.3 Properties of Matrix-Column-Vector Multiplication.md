---
{"dg-publish":true,"permalink":"/linear-algebra/10-matrix-vector-multiplication/10-3-properties-of-matrix-column-vector-multiplication/","tags":["Type/Theorem","Topic/Linear_Algebra"]}
---

Types: *N/A*
Examples: *N/A*
Constructions: *N/A*
Generalizations: *N/A*

Properties: *N/A*
Sufficiencies: *N/A*
Questions: *N/A*

> [!theorem|*] Algebraic Properties of Matrix-Column-Vector Multiplication
> Let matrix $A \in \mathbb{R}^{ m \times n}$, let vectors $\vec{x}, \vec{y} \in \mathbb{R}^{n \times1}$, and let scalars $\alpha, \beta \in \mathbb{R}$. Then, each of the following laws holds true:
> 1. Distributivity: $A(\vec{x} + \vec{y}) = A\vec{x} + A\vec{y}$
> 2. Scalar Multiplication: $A(\alpha \vec{x}) = \alpha(A\vec{x})$
> 3. Linearity: $A(\alpha \vec{x}+\beta \vec{y}) = \alpha A\vec{x} + \beta A\vec{y}$

**Remark.** When trying to prove theorems, we want to transform our theorem into the general form of a conditional statement (read if $P$, then $Q$).
 $$
\underset{ \text{antecedent} }{ P } \implies \underset{ \text{consequent} }{ Q }
$$
- We first assume the antecedent is true.
- Then, the consequent must follow from that assumption.
- If *something* is true, then the *other thing* must also be true.
 <span style='float:right;'>$\blacklozenge$</span>
# Distributivity
For distributivity, we can say
$$
\text{If } A \in \mathbb{R}^{m \times n} \text{ and } \vec{x},\vec{y} \in \mathbb{R}^{n \times 1}, \text{ then } A(\vec{x}+\vec{y}) = A\vec{x}+A\vec{y}
$$
## Concrete Example
Before proving the general case, let's create a concrete example (not a general proof). 
> [!example|*] Distributivity
> Let $m=3$ and $n=4$. Let $A \in \mathbb{R}^{ m \times n}$ and $\vec{x},\vec{y} \in \mathbb{R}^{n \times 1}$.

With our antecedent in mind, we write our matrix
$$
[A]_{m \times n} = \begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} \\
a_{21} & a_{22} & a_{23} & a_{24} \\
a_{31} & a_{32} & a_{33} & a_{34}
\end{bmatrix}_{3 \times 4}
$$
We can also write our vectors
$$
\vec{x} = \begin{bmatrix}
x_{1} \\
x_{2} \\
x_{3} \\
x_{4} 
\end{bmatrix}_{4 \times 1}, \vec{y} = \begin{bmatrix}
y_{1} \\
y_{2} \\
y_{3} \\
y_{4}
\end{bmatrix}_{4 \times 1}
$$
With this information at hand, let's compute our desired result. We want to show
$$
\underbrace{ { A(\vec{x} \cdot \vec{y}) } }_{ \text{expression 1} } = \underbrace{ A \vec{x} + A \vec{y} }_{ \text{expression 2} }
$$
Let's analyze each expression.
$$
\begin{align}
 & \text{Expression 1: } [A]_{m \times n} \cdot ([\vec{x}]_{1 \times n} + [\vec{y}]_{1 \times n}) \\
 & \text{Expression 2: } [A]_{m \times n} \cdot [\vec{x}]_{n \times 1} + [A]_{m \times n} \cdot [\vec{y}]_{n \times 1}
\end{align}
$$

Some theorems/definitions that could be useful are:
- [[Linear Algebra/04 Vector Arithmetic/4.2 Column Vector Addition\|4.2 Column Vector Addition]]
- [[Linear Algebra/10 Matrix-Vector Multiplication/10.1 Matrix-Column-Vector Multiplication via Linear Combinations\|10.1 Matrix-Column-Vector Multiplication via Linear Combinations]]

## Forward Direction
Let's start with expression 1 and work our way towards expression 2.
$$
\begin{align*}
\underbrace{ A \cdot (\vec{x} + \vec{y}) }_{ \text{expression 1} } &= \underbrace{ \begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} \\
a_{21} & a_{22} & a_{23} & a_{24} \\
a_{31} & a_{32} & a_{33} & a_{34} \\
\end{bmatrix} \left( \begin{bmatrix}
x_{1} \\
x_{2} \\
x_{3} \\
x_{4}
\end{bmatrix} + \begin{bmatrix}
y_{1} \\
y_{2} \\
y_{3} \\
y_{4}
\end{bmatrix}\right)  }_{\text{expression 1.1}  } \\
&= \underbrace{ \begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} \\
a_{21} & a_{22} & a_{23} & a_{24} \\
a_{31} & a_{32} & a_{33} & a_{44}
\end{bmatrix}_{m \times n} \cdot \begin{bmatrix}
x_{1}+y_{1} \\
x_{2}+y_{2} \\
x_{3}+y_{3} \\
x_{4}+y_{4}
\end{bmatrix}_{n \times 1} }_{ \text{expression 1.2} }\\
&= \sum_{k=1}^{n} (x_{k} + y_{k}) \cdot A(:, k)\\
&= \underbrace{ (x_{1}+y_{1}) \begin{bmatrix}
a_{11} \\
a_{21} \\
a_{31}
\end{bmatrix} }_{ k=1 }+ \underbrace{ (x_{2}+y_{2})\begin{bmatrix}
a_{12} \\
a_{22} \\
a_{32}
\end{bmatrix} }_{ k = 2 } + \underbrace{ (x_{3}+y_{3})\begin{bmatrix}
a_{13} \\
a_{23} \\
a_{33}
\end{bmatrix} }_{ k = 3 } + \underbrace{ (x_{4} + y_{4}) \begin{bmatrix}
a_{14} \\
a_{24} \\
a_{34}
\end{bmatrix} }_{ k=4 }
\end{align*}
$$
Notice how our sum is composed of [[Linear Algebra/04 Vector Arithmetic/4.1 Scalar-Vector Multiplication\|4.1 Scalar-Vector Multiplication]] products. Consider
$$
\begin{align*}
(x_{1} + y_{1})\begin{bmatrix}
a_{11} \\
a_{21} \\
a_{31}
\end{bmatrix} &= \begin{bmatrix}
(x_{1}+y_{1})a_{11} \\
(x_{1}+y_{1}) a_{21} \\
(x_{1}+y_{1}) a_{31}
\end{bmatrix}\\
&= \begin{bmatrix}
x_{1} a_{11} + y_{1} a_{11} \\
x_{1} a_{21} + y_{1} a_{21} \\
x_{1} a_{31} + y_{1} a_{31}
\end{bmatrix}\\
&= \begin{bmatrix}
x_{1} a_{11} \\
x_{1} a_{21} \\
x_{1}a_{31}
\end{bmatrix} + \begin{bmatrix}
y_{1}a_{11}  \\
y_{1} a_{21}  \\
y_{1} a_{31}
\end{bmatrix}\\
&= x_{1} \begin{bmatrix}
a_{11} \\
a_{21} \\
a_{31}
\end{bmatrix} + y_{1} \begin{bmatrix}
a_{11} \\
a_{21} \\
a_{31}
\end{bmatrix}
\end{align*}
$$
> [!conjecture|*] 
> $$(\alpha + \beta) \cdot \vec{a} = \alpha \vec{a} + \beta \vec{a}$$
> Note: For conjectures, we either prove in general or quote a result (for math heads we prove every result ourselves).

Going back to our original forward direction problem, we can distribute our conjecture across where we stopped previously.
$$
\begin{align*}
&= x_{1} \begin{bmatrix}
a_{11} \\
a_{21} \\
a_{31}
\end{bmatrix} + y_{1} \begin{bmatrix}
a_{11} \\
a_{21} \\
a_{31}
\end{bmatrix} + x_{1} \begin{bmatrix}
a_{12} \\
a_{22}  \\
a_{32}
\end{bmatrix} + y_{1} \begin{bmatrix}
a_{12} \\
a_{22} \\
a_{32}
\end{bmatrix} + x_{1}\begin{bmatrix}
a_{13} \\
a_{23} \\
a_{33}
\end{bmatrix} + y_{1} \begin{bmatrix}
a_{13} \\
a_{23} \\
a_{33}
\end{bmatrix} + x_{1} \begin{bmatrix}
a_{14} \\
a_{24} \\
a_{34}
\end{bmatrix} + y_{1} \begin{bmatrix}
a_{14} \\
a_{24} \\
a_{34}
\end{bmatrix}
\end{align*}
$$
## Backward Direction
Now, lets start from expression 2 and work backwards towards expression 1.
$$
\begin{align*}
&= A\vec{x} + A\vec{y}\\
&= \sum_{k=1}^{n} x_{k} A(:,k) + \sum_{k=1}^{n} y_{k}A(:,k)\\
&= x_{1} \begin{bmatrix}
a_{11} \\
a_{21} \\
a_{31}
\end{bmatrix} + y_{1} \begin{bmatrix}
a_{11} \\
a_{21} \\
a_{31}
\end{bmatrix} + x_{1} \begin{bmatrix}
a_{12} \\
a_{22}  \\
a_{32}
\end{bmatrix} + y_{1} \begin{bmatrix}
a_{12} \\
a_{22} \\
a_{32}
\end{bmatrix} + x_{1}\begin{bmatrix}
a_{13} \\
a_{23} \\
a_{33}
\end{bmatrix} + y_{1} \begin{bmatrix}
a_{13} \\
a_{23} \\
a_{33}
\end{bmatrix} + x_{1} \begin{bmatrix}
a_{14} \\
a_{24} \\
a_{34}
\end{bmatrix} + y_{1} \begin{bmatrix}
a_{14} \\
a_{24} \\
a_{34}
\end{bmatrix}
\end{align*}
$$
We are able to write our sum as such because vector addition is commutative. If we generalize our solution, we will see that
$$
\text{If } A \in \mathbb{R}^{m \times n} \text{ and } \vec{x},\vec{y} \in \mathbb{R}^{n \times 1}, \text{ then } A(\vec{x}+\vec{y}) = A\vec{x}+A\vec{y}
$$

---
# Scalar Multiplication
For this theorem, we might state
$$
\underbrace{ \text{If } A \in \mathbb{R}^{m \times n}, \vec{x} \in \mathbb{R}^{n \times 1}, \text{ and } \alpha \in \mathbb{R}, }_{ P } \text{ then } \underbrace{ A(\alpha \vec{x})  = \alpha(A \vec{x})  }_{ Q }
$$
Some theorems/definitions that could be useful for proving this:
- [[Linear Algebra/04 Vector Arithmetic/4.3 Algebraic Properties of Vector Arithmetic#^420e18\|Algebraic Properties of Vector Addition and Scalar Multiplication]]  
- [[Linear Algebra/10 Matrix-Vector Multiplication/10.2 Matrix-Column-Vector Multiplication via Dot Products#^fed7d7\|Matrix-Column-Vector Multiplication via Dot Products]]
- [[Linear Algebra/03 Vector Modeling/3.5 Equal Column Vectors#^ac16cb\|Equal Column Vectors]] 

We want to prove that two vectors are equal. We know that two vectors are equal if the each entry is to the corresponding entry of the other vector. A way we can get the entry-by-entry definition of matrix-column-vector multiplication is through the inner product definition. With these definitions in mind, let's formalize our proof.

`\begin{proof}`
Suppose that $A \in \mathbb{R}^{m \times n}$, $\vec{x} \in \mathbb{R}^{n}$, and $\alpha \in \mathbb{R}$. For any $i=1,2,\dots,m$, we want to show that $\text{entry}_{(i,1)}(A(\alpha \vec{x})) = \text{entry}_{(i, 1)}(\alpha(A \vec{x}))$. Consider
$$
\begin{align*}
\text{entry}_{(i,1)}(A(a\vec{x})) &= (A(i,:))^{T} \cdot (\alpha \vec{x})\\
\begin{bmatrix}
a_{i1} \\
a_{i 2} \\
\vdots \\
a_{in}
\end{bmatrix} \cdot \begin{bmatrix}
ax_{1} \\
ax_{2} \\
\vdots \\
ax_{n}
\end{bmatrix} &= a_{i 1}(\alpha x_{1}) + a_{i 2} (\alpha x_{2}) + \dots + a_{in} (\alpha x_{n})\\
&= \alpha(a_{ i 1} + a_{i 2} + \dots + a _{i n})\\
&= \alpha \left( (A(i,:))^{T} \cdot \vec{x} \right)\\
&= \text{entry}_{(i, 1)} (\alpha(A \vec{x}))

\end{align*}
$$
This is exactly what we wanted to show.

`\end{proof}`